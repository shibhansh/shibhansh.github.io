<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Shibhansh Dohare</title>
  
  <meta name="author" content="Shibhansh Dohare">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Shibhansh Dohare</name>
              </p>
              <p>
                I am a final year Ph.D. student at the University of Alberta,
                advised by <a href="http://incompleteideas.net/">Dr. Richard Sutton</a> and <a href="https://armahmood.github.io/">Dr. Rupam Mahmood</a>.
                I completed my B.Tech. at IIT Kanpur in Computer Science and Engineering.
                My long-term research goal is to understand the workings of our minds.
                Specifically, to help find the computational principles that give rise to the mind.
                In pursuit of this goal, I'm working on various aspects of continual learning, deep learning, and reinforcement learning.
              <p>
                During my Ph.D., I have contributed to exposing a fundamental problem with deep learning systems,
                where these systems can lose the ability to learn new things.
                I also developed the continual backpropagation algorithm to overcome this problem.
                My Ph.D. research has been published in <a href="https://www.nature.com/articles/s41586-024-07711-7">Nature</a> and featured in some popular media outlets,
                such as <a href="https://www.newscientist.com/article/2444870-ai-models-cant-learn-as-they-go-along-like-humans-do/">New Scientist</a>.
                If you prefer podcasts, I have also discussed my work on the <a href="https://www.nature.com/articles/d41586-024-02756-0">Nature Podcast</a> and AMII's <a href="https://open.spotify.com/episode/4GFXGRg9ZiNTZdTfDXKAdq?si=cPyVtlNPRpyDo1Vw-IowOA">Approximately Correct Podcast</a>.
              </p>
              <p>
                <b>I am on the job market for Spring/Summer 2025</b>. Feel free to reach out if you think I‚Äôd be a good fit for your research position.
              </p>
              <p style="text-align:center">
                <a href="mailto:dohare@ualberta.ca">Email</a> &nbsp/&nbsp
                <a href="data/shibhansh_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=mqkvfUkAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/s_dohare">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/shibhansh/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/shibhansh.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/shibhansh.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

<!--        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>-->
<!--            <tr>-->
<!--            <td style="padding:20px;width:10%;vertical-align:middle">-->
<!--              <heading>News</heading>-->
<!--            <tr>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; [Aug 2024]</td>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; Our paper on <a href="https://www.nature.com/articles/s41586-024-07711-7">Loss of plasticity in deep continual learning</a> is now published in <em>Nature</em>.-->
<!--            </tr>-->
<!--            <tr> <td> </td> <td> </td> </tr>-->
<!--            <tr> <td> </td> <td> </td> </tr>-->
<!--            <tr>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; [Sept 2023]</td>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; We published a <a href="https://openreview.net/forum?id=m9Jfdz4ymO">paper</a> on <em>policy collapse</em> in EWLR 2023.-->
<!--            </tr>-->
<!--            <tr> <td> </td> <td> </td> </tr>-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; [Aug 2023]</td>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; Our <a href="https://arxiv.org/abs/2306.13812">paper</a>, <em>Loss of Plasticity in Deep Continual Learning</em>, is finally available on arxiv.&ndash;&gt;-->
<!--&lt;!&ndash;            </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--            <tr> <td> </td> <td> </td> </tr>-->
<!--&lt;!&ndash;            <tr>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; [Oct 2022]</td>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; Released a <a href="https://github.com/shibhansh/loss-of-plasticity">repo</a> containing supervised learning problems where we can study loss of plasticity.</td>&ndash;&gt;-->
<!--&lt;!&ndash;            </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--            <tr>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; [Aug 2022]</td>-->
<!--              <td>&emsp;&emsp;&emsp;&emsp; I shared a <a href="https://www.youtube.com/watch?v=p_zknyfV9fY">keynote</a> keynote at <a href="https://lifelong-ml.cc/">CoLLAs</a> with Rich on <em>Maintaining Plasticity in Deep Continual Learning</em>.</td>-->
<!--            </tr>-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr> <td> </td> <td> </td> </tr>&ndash;&gt;-->
<!--&lt;!&ndash;            <tr>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; [Apr 2022]</td>&ndash;&gt;-->
<!--&lt;!&ndash;              <td>&emsp;&emsp;&emsp;&emsp; I presented our <a href="https://arxiv.org/abs/2108.06325v3">paper</a>, <em>Continual Backprop: Stochastic Gradient Descent with Persistent Randomness</em>, at <a href="https://rldm.org/">RLDM</a>.</td>&ndash;&gt;-->
<!--&lt;!&ndash;            </tr>&ndash;&gt;-->
<!--         </table>-->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
<!--                My favorite papers are <span class="highlight">highlighted</span>.-->
              </p>
            </td>
          </tr>
        </tbody>
        </table>

        <table style="width:120%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

<!--          <tr onmouseout="lop_stop()" onmouseover="lop_start()"  bgcolor="#ffffd0">-->
            <td style="padding:0px;width:25%;vertical-align:top">
              <div class="one">
                <img src='images/lop.png' width="213">
              </div>
              <script type="text/javascript">
                function lop_start() {
                  document.getElementById('lop_image').style.opacity = "1";
                }

                function lop_stop() {
                  document.getElementById('lop_image').style.opacity = "0";
                }
                lop_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle> <a href="https://www.nature.com/articles/s41586-024-07711-7">Loss of Plasticity in Deep Continual Learning</a></papertitle>
              </a>
              <br>
                <strong>Shibhansh Dohare</strong>,
              <a href="https://scholar.google.ca/citations?user=vD3XXGwAAAAJ&hl=en">J. Fernando Hernandez-Garcia</a>,
              <a href="https://lancelqf.github.io/about/">Qingfeng Lan</a>,
              <a href="https://parashrahman.github.io/">Parash Rahman</a>,
              <a href="https://armahmood.github.io/">A. Rupam Mahmood</a>,
              <a href="http://incompleteideas.net/">Richard S. Sutton</a>
              <br>
              <em>Nature</em> 2024
              <br>
              <a href="https://www.nature.com/articles/s41586-024-07711-7">Paper</a>
		|
              <a href="https://github.com/shibhansh/loss-of-plasticity">Code</a>
                |
              <a href="https://www.youtube.com/watch?v=Zp_a9M309k4">Nature Podcast</a>
                |
              <a href="https://www.nature.com/articles/d41586-024-02525-z">News</a>
              <p></p>
              <p>
                  We provide first direct demonstrations of plasticity loss in deep continual learning.
              </p>
              <p>
                  We propose a new algorithm, <em>continual backpropagation</em>, that fully maintains plasticity.
                  Continual backpropagation re-initializes a small fraction of less-used units alongside gradient descent at each update.
              </p>
            </td>
          </tr>

          <tr onmouseout="pc_stop()" onmouseover="pc_start()">
            <td style="padding:20px;width:25%;vertical-align:bottom">
              <div class="one">
                <img src='images/pc.png' width="200">
              </div>
              <script type="text/javascript">
                function pc_start() {
                  document.getElementById('pc_image').style.opacity = "1";
                }

                function pc_stop() {
                  document.getElementById('pc_image').style.opacity = "0";
                }
                pc_stop()
              </script>
            </td>

            <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Overcoming Policy Collapse in Deep Reinforcement Learning</papertitle>
              </a>
              <br>
                <strong>Shibhansh Dohare</strong>,
              <a href="https://lancelqf.github.io/about/">Qingfeng Lan</a>,
              <a href="https://armahmood.github.io/">A. Rupam Mahmood</a>
              <br>
              <em>EWRL 2023</em>
              <br>
              <a href="https://openreview.net/forum?id=m9Jfdz4ymO">Paper</a>
              <p></p>
              <p>
We show that popular deep RL algorithms, like PPO, do not scale with experience. Their performance gets worse over time. We look deeper into this problem and provide simple solutions to reduce performance degradation.              </p>
            </td>
          </tr>

          <tr onmouseout="gamma_stop()" onmouseover="gamma_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='anf_image'>
                  <img src='images/noisy_env.png' width="200"></div>
                <img src='images/noisy_env.png' width="200">
              </div>
              <script type="text/javascript">
                function gamma_start() {
                  document.getElementById('anf_image').style.opacity = "1";
                }

                function gamma_stop() {
                  document.getElementById('anf_image').style.opacity = "0";
                }
                gamma_stop()
              </script>
            </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
              <a>
                <papertitle>Automatic Noise Filtering with Dynamic Sparse Training in Deep Reinforcement Learning</papertitle>
              </a>
              <br>
                          <a href="https://www.bramgrooten.nl/">Bram Grooten</a>,
                          <a href="https://research.tue.nl/en/persons/ghada-sokar">Ghada Sokar</a>,
                          <strong>Shibhansh Dohare</strong>,
                          <a href="https://people.utwente.nl/e.mocanu">Elena Mocanu</a>,
                          <a href="https://drmatttaylor.net/">Matthew E. Taylor</a>,
                          <a href="https://www.tue.nl/en/research/researchers/mykola-pechenizkiy/">Mykola Pechenizkiy</a>,
                          <a href="https://www.uni.lu/fstm-en/people/decebal-constantin-mocanu/">Decebal Constantin Mocanu</a>
              <br>
              <em>AAMAS 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2302.06548">Paper</a>
              <p></p>
              <p>
                  We show that standard Deep RL algorithms fail when the input contains noisy features.
                  Dynamic sparse training successfully filters through the noisy features and performs well.
              </p>
            </td>
          </tr>

          <tr onmouseout="gamma_stop()" onmouseover="gamma_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='gamma_image'>
                  <img src='images/gamma_net.png' width="200"></div>
                <img src='images/gamma_net.png' width="200">
              </div>
              <script type="text/javascript">
                function gamma_start() {
                  document.getElementById('gamma_image').style.opacity = "1";
                }

                function gamma_stop() {
                  document.getElementById('gamma_image').style.opacity = "0";
                }
                gamma_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Gamma-Nets: Generalizing Value Estimation over Timescale</papertitle>
              </a>
              <br>
              <a href="https://scholar.google.ca/citations?user=z_UYQB8AAAAJ&hl=en">Craig Sherstan</a>,
              <strong>Shibhansh Dohare</strong>,
              <a href="https://scholar.google.com/citations?user=Gx0PQ9kAAAAJ&hl=en">James MacGlashan</a>,
              <a href="https://scholar.google.ca/citations?user=Wrmm6fcAAAAJ&hl=en">Johannes G√ºnther</a>,
              <a href="https://sites.ualberta.ca/~pilarski/">Patrick M. Pilarski</a>,
              <br>
              <em>AAAI, Oral Presentation</em>, 2020
              <br>
              <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6027">Paper</a>
              <p></p>
              <p>
                We present Gamma-nets, a method for generalizing value function estimation over timescale.
              </p>
            </td>
          </tr>

          <tr onmouseout="amr_stop()" onmouseover="amr_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='amr_image'>
                  <img src='images/amr.png' width="160"></div>
                <img src='images/amr.png' width="160">
              </div>
              <script type="text/javascript">
                function amr_start() {
                  document.getElementById('amr_image').style.opacity = "1";
                }

                function amr_stop() {
                  document.getElementById('amr_image').style.opacity = "0";
                }
                amr_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Unsupervised Semantic Abstractive Summarization</papertitle>
              </a>
              <br>
              <strong>Shibhansh Dohare</strong>,
              <a href="https://vgupta123.github.io/">Vivek Gupta</a>,
              <a href="https://www.linkedin.com/in/harish-karnick-388b1616/?originalSubdomain=in">Harish Karnick</a>,
              <br>
              <em>ACL, Student Research Workshop</em>, 2018
              <br>
              <a href="https://aclanthology.org/P18-3011/">Paper</a>
              <p></p>
              <p>
                A novel algorithm for abstractive text summarization based on Abstract Meaning Representation.
              </p>
            </td>
          </tr>		

        </tbody>
        </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website design credit goes to <a href="https://jonbarron.info/">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
